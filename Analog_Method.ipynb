{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import iris\n",
    "import iris.plot as iplt\n",
    "import matplotlib.pyplot as plt\n",
    "import cloudpickle\n",
    "import pandas as pd\n",
    "import time\n",
    "from eofs.iris import Eof\n",
    "from eofs.multivariate.iris import MultivariateEof\n",
    "import datetime\n",
    "import numpy as np\n",
    "# import netcdf4 as nc4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "\n",
    "1. __Ausschneiden der geographischen Region__:<br>\n",
    "    Relevante geographische Region:(ùù∫ = [10 ¬∞W, 25 ¬∞E],ùûÖ = [32.5 ¬∞N, 67.5 ¬∞N)<br>\n",
    "    Dieser Schritt wurde bereits beim Datendownload ber√ºcksichtigt.\n",
    "    \n",
    "    \n",
    "2. __Berechnen & Normieren der Anomalien__:<br>\n",
    "    Diese Aufgabe wurde in der folgenden Funktion _preprocessing_ ausgef√ºhrt.<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(data):\n",
    "    # Berechnen des Mittelwerts\n",
    "    mean=data.rolling(time=21,center=True).mean().dropna('time')\n",
    "    climatology=mean.groupby(\"time.dayofyear\").mean('time')\n",
    "    \n",
    "    # Berechnen der Standardabweichung \n",
    "    data_for_std=data.rolling(time=21,center=True).construct(\"rolling_days\")\n",
    "    std_dayofyear=data_for_std.groupby(\"time.dayofyear\").std(dim=xr.ALL_DIMS)\n",
    "    \n",
    "    # Berechnen der Anomalien\n",
    "    diff=data.groupby(\"time.dayofyear\")-climatology\n",
    "    anomalies=diff.groupby(\"time.dayofyear\")/std_dayofyear\n",
    "    return anomalies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data=xr.open_dataset(\"./small_daily_data.nc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Berechnen der Anomalien:\n",
    "# anomalies=preprocessing(data)\n",
    "\n",
    "# # Speichern der Anomalien:\n",
    "# data = Dataset(\"/home/vis/anomalies_1979-2018.nc4\", 'w', format='NETCDF4')\n",
    "# data = anomalies\n",
    "# data.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# √ñffnen der gespeicherten Daten:\n",
    "#path=/home/vis/\n",
    "path=\"/mnt/c/Users/Kathi/Documents/Studium/Master/Klima/√úbung/Projekt/\"\n",
    "anomalies=xr.open_dataset(path+\"anom_all_1979-2018.nc\",chunks={'time': 1, 'lat':141, 'lon': 141})\n",
    "print(anomalies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testplot der Anomalien\n",
    "plt.plot(anomalies.r.sel(time=slice(\"1979-01-01\",\"1981-01-01\"),lat=47.0,lon=12.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analog Methode\n",
    "\n",
    "## Warum?\n",
    "Der Skillfull Scale eines globalen Modells, insbesondere eines globalen Klimamodells, liegt bei ca. 800 - 1000 km. Dadurch kann man nicht einfach z.B. globale Klimaprognosen mit regionale Auswirkungen gleichsetzen. <br>\n",
    "Dazu wird ein Downscaling ben√∂tigt, wobei man dabei zwei Arten unterscheidet:\n",
    "1. Dynamisches Downscaling\n",
    "2. Statistisches Downscaling\n",
    "\n",
    "Bei Ersterem wird in das globale Modell ein regionales genestet, was sehr rechenintensiv ist. Bei statistischem Downscaling geht es darum, mit statistischen Hilfsmitteln, Transferfunktionen zu finden, die das globale Klimamodell auf die regionale Skala transformieren, siehe folgende Abbildung:\n",
    "\n",
    "<img src=\"lasclosc.png\" alt=\"Statistical Downscaling\" style=\"width: 600px;\" />\n",
    "\n",
    "## Was ist die Analog - Methode?\n",
    "\n",
    "Um den Transfer von globaler auf lokale Skala zu vollziehen, wir hier die Empirical Orthogonal Function (EOF) - Analyse ben√ºtzt.<br>\n",
    "Die Idee hinter der EOF-Analyse ist es, ein Feld durch eine wesentlich geringer Menge an Daten wie das Original auszudr√ºcken, ohne dabei gro√üe Einbu√üen in der Information zu erzeugen. Dabei werden sowohl r√§umliche (EOF) und zeitliche (Prinicipal Component - PC) Muster aus den Daten bestimmt. <br>\n",
    "\n",
    "Zum Veranschaulichen dieser Methode wird ein Testjahr (*targetyear*) ausgew√§hlt, hier 2015. F√ºr jeden Tag dieses Jahres (*dayofyear (doy)*) wird ein Analog-Tag gesucht. Da z.B. f√ºr den 15. J√§nner nicht ein Tag im Juli der perfekte Analog-Tag sein wird, wird f√ºr die EOF-Analyse jeweils nur ein Zeitfenster von $\\pm$10 Tagen um den gesuchten Tag ausgew√§hlt, f√ºr alle verf√ºgbaren Jahre. Auf die resultierenden EOFs wird das Feld des gesuchten Tages projeziert wodurch man Pseudo-PCs (PPC) erh√§lt. <br>\n",
    "\n",
    "Um den √§hnlichsten Tag f√ºr das Analogon zu finden wird die Norm zwischen den aus der EOF-Analyse gewonnen PCs und den Pseudo-PCS gebildet, nach der Formel:\n",
    "\n",
    "\\begin{equation}\n",
    "\\text{Norm}=\\sum{(\\text{PPC}-\\text{PC})^2}\n",
    "\\end{equation}\n",
    "\n",
    "Bei der Normbildung muss nat√ºrlich aus dem Pool der aktuelle Tag herausgenommen werden, da dieser ja gleich zum Targetday ist.\n",
    "Um eine gr√∂√üere Auswahl zu haben wird nicht nur der Tag mit der geringsten Norm ausgew√§hlt, sondern die kleinsten x-Tage. x kann beliebig gew√§hlt werden und wird in diesem Projekt mit 3 angenommen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Die Idee hinter einer EOF - Analyse ist, den selben Informationsgehalt mit weniger Daten auszudr√ºcken.# Pfad des Speicherortes der Ausgabedaten\n",
    "#path = \"/home/vis/\"\n",
    "path=\"/mnt/c/Users/Kathi/Documents/Studium/Master/Klima/√úbung/Projekt/\"\n",
    "\n",
    "#  Jahr festlegen, f√ºr welches Analoga erstellt werden sollen\n",
    "analogyear = '2015'\n",
    "targetyear = pd.date_range(analogyear+'-01-01', analogyear+'-12-31')\n",
    "\n",
    "# Wie viele Analoga sollen gesucht werden:\n",
    "nr = 3\n",
    "\n",
    "# L√§nge der Zeitreihe\n",
    "years = len(anomalies.groupby('time.year'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Testjahr: ',analogyear)\n",
    "print('Analoga: ',nr)\n",
    "print('Jahre: ',years)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Ausgabelisten erstellen\n",
    "out_pc = []\n",
    "out_r = []\n",
    "out_q = []\n",
    "out_msl = []\n",
    "\n",
    "# Ausgabe Array f√ºr die Analoga erstellen:\n",
    "out_analoga = np.zeros((len(anomalies.time), nr+1), dtype='datetime64[s]')\n",
    "# Der ersten Spalte werden die Zeitelement zugeordnet -> Tage\n",
    "out_analoga[:,0] = anomalies.time\n",
    "\n",
    "\n",
    "# √úber alle DOY iterieren:\n",
    "for i, day in enumerate(targetyear):\n",
    "    \n",
    "    # Fenster erstellen: 10 Tage vor und nach dem aktuellen + alle doy ausw√§hlen\n",
    "    window = pd.date_range(day - datetime.timedelta(days=10), day + datetime.timedelta(days=10))\n",
    "    doy_window = window.dayofyear\n",
    "\n",
    "    # Alle Daten welche sich im Fenster befinden ausw√§hlen und f√ºr die EOF-Analyse time attributes = 'T' setzen\n",
    "    data = anomalies.sel(time = anomalies.time.dt.dayofyear.isin(doy_window))\n",
    "    data.coords['time'].attrs['axis'] = 'T'\n",
    "    \n",
    "    # Daten des aktuellen Tag ausw√§hlen -> f√ºr die PPC-Berechnung\n",
    "    data_doy = anomalies.sel(time = anomalies.time.dt.dayofyear.isin(day.dayofyear))\n",
    "\n",
    "    # F√ºr die Analyse ist es notwendig die Daten in das Iris Format zu wandeln:\n",
    "    data_iris = iris.cube.CubeList([data.r.to_iris(), data.q.to_iris(), data.msl.to_iris()]).merge() \n",
    "    \n",
    "    # EOF-Analyse starten:\n",
    "    solver = MultivariateEof(data_iris, weights=None)\n",
    "    \n",
    "    # PC berechnen, bis 90% der Varianz erkl√§rt sind\n",
    "    j = 0\n",
    "    while(True):\n",
    "        variance = solver.varianceFraction(neigs=j)\n",
    "        if(np.sum(variance.data) > 0.9):\n",
    "            break\n",
    "        j += 1\n",
    "    \n",
    "    # Gefunden PCs und EOFs ausw√§hlen:\n",
    "    pc = solver.pcs(npcs=j)\n",
    "    eof = solver.eofs(neofs=j)\n",
    "    \n",
    "    # R√ºcktransformation nach fertiger EOF-Analyse und Fertige Berechnung zur Liste hinzuf√ºgen -> PC f√ºr die Normberechnung sp√§ter noch notwenig! \n",
    "    calculated_pc = xr.DataArray.from_iris(pc).rename('PC')\n",
    "    out_pc.append(calculated_pc)\n",
    "    out_r.append(xr.DataArray.from_iris(eof[0]).rename('EOF_r'))\n",
    "    out_q.append(xr.DataArray.from_iris(eof[1]).rename('EOF_q'))\n",
    "    out_msl.append(xr.DataArray.from_iris(eof[2]).rename('EOF_msl'))\n",
    "   \n",
    "    # F√ºr die Analyse ist es auch hier wieder notwendig die Daten des aktuellen DOY in das Iris Format zu wandeln:\n",
    "    data_doy_iris = iris.cube.CubeList([data_doy.r.to_iris(), data_doy.q.to_iris(), data_doy.msl.to_iris()]).merge()    \n",
    "\n",
    "    # Berechnen der PPCs:\n",
    "    ppc = solver.projectField(data_doy_iris, neofs=j)\n",
    "\n",
    "    # R√ºcktransformation:\n",
    "    data_ppc = xr.DataArray.from_iris(ppc).rename('PPC')\n",
    "\n",
    "    # √úber alle Jahre iterieren:\n",
    "    for k in range(0, years):\n",
    "        \n",
    "        # Berechnen der Norm\n",
    "        norm = (( calculated_pc - data_ppc[k] )**2).sum(dim='pc')\n",
    "        \n",
    "        # Das akutelle Jahr aus dem Pool herausnehmen\n",
    "        norm = norm.sel(time = ~norm.time.dt.year.isin(data_ppc.time.to_series()[k].year))\n",
    "        kday = data_ppc.time.to_series().dt.strftime('%Y-%m-%d')[k]\n",
    "        \n",
    "        # Reihe im Ausgabe Array festlegen\n",
    "        row = np.where(out_analoga[:,0] == datetime.datetime.strptime(kday, '%Y-%m-%d'))[0][0]\n",
    "\n",
    "        #Suche nach dem Analogon f√ºr den aktuellen Tag (Anzahl oben festgelegt -> nr)\n",
    "        for l in range(1,nr+1):\n",
    "            \n",
    "            # Norm minimieren -> Analogen finden: kleinste Norm = beste √úbereinstimmung\n",
    "            mini = np.argmin(norm).values\n",
    "            \n",
    "            # Das k.te Jahr auslblenden und Analogon bestimmen\n",
    "            analogon = data.sel(time = ~data.time.dt.year.isin(data_ppc.time.to_series()[k].year)).time.to_series()[mini]\n",
    "            \n",
    "            # Analogon im Array ablegen\n",
    "            out_analoga[row][l] = analogon\n",
    "            \n",
    "            # Das bereits gefunden Minimum herausnehmen, um die weiteren finden zu k√∂nnen:\n",
    "            norm = norm.where(norm.values != norm[mini].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Speichern der Ausgangsdaten:\n",
    "cloudpickle.dump( out_r, open( path + \"/EOFr_\" + analogyear + \".p\", \"wb\" ) )\n",
    "cloudpickle.dump( out_q, open( path + \"/EOFq_\" + analogyear + \".p\", \"wb\" ) )\n",
    "cloudpickle.dump( out_msl, open( path + \"/EOFmsl_\" + analogyear + \".p\", \"wb\" ) )\n",
    "cloudpickle.dump( out_analoga, open( path + \"/Analoga_\" + analogyear + \".p\", \"wb\" ) )\n",
    "cloudpickle.dump( out_pc, open( path + \"/PC_\" + analogyear + \".p\", \"wb\" ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die meiste Arbeit war das Verstehen der Aufgabenstellung und das Implementieren dieser. Auch die Konvertierung in Iris hat einige Fragen aufgeworfen, war aber durch Cf - Konformit√§t der ERA5-Daten nich so problematisch wie bei anderen Grupppen."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
