{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vis/anaconda3/envs/klima/lib/python3.7/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n",
      "  data = yaml.load(f.read()) or {}\n",
      "/home/vis/anaconda3/envs/klima/lib/python3.7/site-packages/distributed/config.py:20: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n",
      "  defaults = yaml.load(f)\n"
     ]
    }
   ],
   "source": [
    "import xarray as xr\n",
    "import iris\n",
    "import iris.plot as iplt\n",
    "import matplotlib.pyplot as plt\n",
    "import cloudpickle\n",
    "import pandas as pd\n",
    "import time\n",
    "from eofs.iris import Eof\n",
    "from eofs.multivariate.iris import MultivariateEof\n",
    "import datetime\n",
    "import numpy as np\n",
    "# import netcdf4 as nc4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "\n",
    "1. __Ausschneiden der geographischen Region__:<br>\n",
    "    Relevante geographische Region:(ùù∫ = [10 ¬∞W, 25 ¬∞E],ùûÖ = [32.5 ¬∞N, 67.5 ¬∞N)<br>\n",
    "    Dieser Schritt wurde bereits beim Datendownload ber√ºcksichtigt.\n",
    "    \n",
    "    \n",
    "2. __Berechnen & Normieren der Anomalien__:<br>\n",
    "    Diese Aufgabe wurde in der folgenden Funktion _preprocessing_ ausgef√ºhrt.<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(data):\n",
    "    # Berechnen des Mittelwerts\n",
    "    mean=data.rolling(time=21,center=True).mean().dropna('time')\n",
    "    climatology=mean.groupby(\"time.dayofyear\").mean('time')\n",
    "    \n",
    "    # Berechnen der Standardabweichung \n",
    "    data_for_std=data.rolling(time=21,center=True).construct(\"rolling_days\")\n",
    "    std_dayofyear=data_for_std.groupby(\"time.dayofyear\").std(dim=xr.ALL_DIMS)\n",
    "    \n",
    "    # Berechnen der Anomalien\n",
    "    diff=data.groupby(\"time.dayofyear\")-climatology\n",
    "    anomalies=diff.groupby(\"time.dayofyear\")/std_dayofyear\n",
    "    return anomalies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data=xr.open_dataset(\"./small_daily_data.nc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Berechnen der Anomalien:\n",
    "# anomalies=preprocessing(data)\n",
    "\n",
    "# # Speichern der Anomalien:\n",
    "# data = Dataset(\"/home/vis/anomalies_1979-2018.nc4\", 'w', format='NETCDF4')\n",
    "# data = anomalies\n",
    "# data.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: b'/mnt/c/Users/Kathi/Documents/Studium/Master/Klima/\\xc3\\x9cbung/Projekt/anom_all_1979-2018.nc'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/envs/klima/lib/python3.7/site-packages/xarray/backends/file_manager.py\u001b[0m in \u001b[0;36macquire\u001b[0;34m(self, needs_lock)\u001b[0m\n\u001b[1;32m    166\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 167\u001b[0;31m                 \u001b[0mfile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cache\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_key\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    168\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/klima/lib/python3.7/site-packages/xarray/backends/lru_cache.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cache\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmove_to_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: [<function _open_netcdf4_group at 0x7f9f66fb1d90>, ('/mnt/c/Users/Kathi/Documents/Studium/Master/Klima/√úbung/Projekt/anom_all_1979-2018.nc', CombinedLock([<SerializableLock: 34062db1-920e-4eed-a9e5-c164d6f7248f>, <SerializableLock: f07f8256-33b4-4782-8d67-828794468962>])), 'r', (('clobber', True), ('diskless', False), ('format', 'NETCDF4'), ('group', None), ('persist', False))]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-7bc879e259db>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m#path=/home/vis/\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"/mnt/c/Users/Kathi/Documents/Studium/Master/Klima/√úbung/Projekt/\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0manomalies\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mxr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"anom_all_1979-2018.nc\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mchunks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'time'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'lat'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m141\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'lon'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m141\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manomalies\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/klima/lib/python3.7/site-packages/xarray/backends/api.py\u001b[0m in \u001b[0;36mopen_dataset\u001b[0;34m(filename_or_obj, group, decode_cf, mask_and_scale, decode_times, autoclose, concat_characters, decode_coords, engine, chunks, lock, cache, drop_variables, backend_kwargs, use_cftime)\u001b[0m\n\u001b[1;32m    418\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'netcdf4'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    419\u001b[0m             store = backends.NetCDF4DataStore.open(\n\u001b[0;32m--> 420\u001b[0;31m                 filename_or_obj, group=group, lock=lock, **backend_kwargs)\n\u001b[0m\u001b[1;32m    421\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'scipy'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    422\u001b[0m             \u001b[0mstore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbackends\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mScipyDataStore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename_or_obj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mbackend_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/klima/lib/python3.7/site-packages/xarray/backends/netCDF4_.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(cls, filename, mode, format, group, clobber, diskless, persist, lock, lock_maker, autoclose)\u001b[0m\n\u001b[1;32m    358\u001b[0m             kwargs=dict(group=group, clobber=clobber, diskless=diskless,\n\u001b[1;32m    359\u001b[0m                         persist=persist, format=format))\n\u001b[0;32m--> 360\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmanager\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlock\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlock\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mautoclose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mautoclose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    361\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/klima/lib/python3.7/site-packages/xarray/backends/netCDF4_.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, manager, lock, autoclose)\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_manager\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmanager\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    320\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_remote\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mis_remote_uri\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/klima/lib/python3.7/site-packages/xarray/backends/netCDF4_.py\u001b[0m in \u001b[0;36mds\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    362\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    363\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mds\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 364\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    365\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    366\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mopen_store_variable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/klima/lib/python3.7/site-packages/xarray/backends/file_manager.py\u001b[0m in \u001b[0;36macquire\u001b[0;34m(self, needs_lock)\u001b[0m\n\u001b[1;32m    171\u001b[0m                     \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m                     \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'mode'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 173\u001b[0;31m                 \u001b[0mfile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_opener\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    174\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m                     \u001b[0;31m# ensure file doesn't get overriden when opened again\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/klima/lib/python3.7/site-packages/xarray/backends/netCDF4_.py\u001b[0m in \u001b[0;36m_open_netcdf4_group\u001b[0;34m(filename, lock, mode, group, **kwargs)\u001b[0m\n\u001b[1;32m    250\u001b[0m     \u001b[0;32mimport\u001b[0m \u001b[0mnetCDF4\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnc4\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 252\u001b[0;31m     \u001b[0mds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnc4\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    253\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mclose_on_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mnetCDF4/_netCDF4.pyx\u001b[0m in \u001b[0;36mnetCDF4._netCDF4.Dataset.__init__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mnetCDF4/_netCDF4.pyx\u001b[0m in \u001b[0;36mnetCDF4._netCDF4._ensure_nc_success\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: b'/mnt/c/Users/Kathi/Documents/Studium/Master/Klima/\\xc3\\x9cbung/Projekt/anom_all_1979-2018.nc'"
     ]
    }
   ],
   "source": [
    "# √ñffnen der gespeicherten Daten:\n",
    "#path=/home/vis/\n",
    "path=\"/mnt/c/Users/Kathi/Documents/Studium/Master/Klima/√úbung/Projekt/\"\n",
    "anomalies=xr.open_dataset(path+\"anom_all_1979-2018.nc\",chunks={'time': 1, 'lat':141, 'lon': 141})\n",
    "print(anomalies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testplot der Anomalien\n",
    "plt.plot(anomalies.r.sel(time=slice(\"1979-01-01\",\"1981-01-01\"),lat=47.0,lon=12.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analog Methode\n",
    "\n",
    "## Warum?\n",
    "Der Skillfull Scale eines globalen Modells, insbesondere eines globalen Klimamodells, liegt bei ca. 800 - 1000 km. Dadurch kann man nicht einfach z.B. globale Klimaprognosen mit regionale Auswirkungen gleichsetzen. <br>\n",
    "Dazu wird ein Downscaling ben√∂tigt, wobei man dabei zwei Arten unterscheidet:\n",
    "1. Dynamisches Downscaling\n",
    "2. Statistisches Downscaling\n",
    "\n",
    "Bei Ersterem wird in das globale Modell ein regionales genestet, was sehr rechenintensiv ist. Bei statistischem Downscaling geht es darum, mit statistischen Hilfsmitteln, Transferfunktionen zu finden, die das globale Klimamodell auf die regionale Skala transformieren, siehe folgende Abbildung:\n",
    "\n",
    "<img src=\"lasclosc.png\" alt=\"Statistical Downscaling\" style=\"width: 600px;\" />\n",
    "\n",
    "## Was ist die Analog - Methode?\n",
    "\n",
    "Um den Transfer von globaler auf lokale Skala zu vollziehen, wir hier die Empirical Orthogonal Function (EOF) - Analyse ben√ºtzt.<br>\n",
    "Die Idee hinter der EOF-Analyse ist es, ein Feld durch eine wesentlich geringer Menge an Daten wie das Original auszudr√ºcken, ohne dabei gro√üe Einbu√üen in der Information zu erzeugen. Dabei werden sowohl r√§umliche (EOF) und zeitliche (Prinicipal Component - PC) Muster aus den Daten bestimmt. <br>\n",
    "\n",
    "F√ºr die sp√§tere Validierung werden hier alle Tage aller Jahre berechnet. F√ºr jeden Tag werden einige (hier 3) Analog-Tage gesucht. Da z.B. f√ºr den 15. J√§nner nicht ein Tag im Juli der perfekte Analog-Tag sein wird, wird f√ºr die EOF-Analyse jeweils nur ein Zeitfenster von $\\pm$10 Tagen um den gesuchten Tag ausgew√§hlt, f√ºr alle verf√ºgbaren Jahre. Auf die resultierenden EOFs wird das Feld des gesuchten Tages projeziert wodurch man Pseudo-PCs (PPC) erh√§lt. <br>\n",
    "\n",
    "Um den √§hnlichsten Tag f√ºr das Analogon zu finden wird die Norm zwischen den aus der EOF-Analyse gewonnen PCs und den Pseudo-PCS gebildet, nach der Formel:\n",
    "\n",
    "\\begin{equation}\n",
    "\\text{Norm}=\\sum{(\\text{PPC}-\\text{PC})^2}\n",
    "\\end{equation}\n",
    "\n",
    "Bei der Normbildung muss nat√ºrlich aus dem Pool der aktuelle Tag herausgenommen werden, da dieser ja gleich zum Targetday ist.\n",
    "Um eine gr√∂√üere Auswahl zu haben wird nicht nur der Tag mit der geringsten Norm ausgew√§hlt, sondern die kleinsten x-Tage. x kann beliebig gew√§hlt werden und wird in diesem Projekt mit 3 angenommen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Die Idee hinter einer EOF - Analyse ist, den selben Informationsgehalt mit weniger Daten auszudr√ºcken.# Pfad des Speicherortes der Ausgabedaten\n",
    "#path = \"/home/vis/\"\n",
    "path=\"/mnt/c/Users/Kathi/Documents/Studium/Master/Klima/√úbung/Projekt/\"\n",
    "\n",
    "#  Beispieljahr erstellen (kein Schaltjahr)\n",
    "exampleyear = pd.date_range('1999-01-01', '1999-12-31')\n",
    "\n",
    "# Wie viele Analoga sollen gesucht werden:\n",
    "nr = 3\n",
    "\n",
    "# L√§nge der Zeitreihe\n",
    "years = len(anomalies.groupby('time.year'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Analoga: ',nr)\n",
    "print('Jahre: ',years)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ausgabelisten erstellen\n",
    "out_pc = []\n",
    "out_r = []\n",
    "out_q = []\n",
    "out_msl = []\n",
    "\n",
    "# Ausgabe Array f√ºr die Analoga erstellen:\n",
    "out_analoga = np.zeros((len(anomalies.time), nr+1), dtype='datetime64[s]')\n",
    "# Der ersten Spalte werden die Zeitelement zugeordnet -> Tage\n",
    "out_analoga[:,0] = anomalies.time\n",
    "\n",
    "\n",
    "# √úber alle DOY iterieren:\n",
    "for i, day in enumerate(exampleyear):\n",
    "    \n",
    "    # Fenster erstellen: 10 Tage vor und nach dem aktuellen + alle doy ausw√§hlen\n",
    "    window = pd.date_range(day - datetime.timedelta(days=10), day + datetime.timedelta(days=10))\n",
    "    doy_window = window.dayofyear\n",
    "\n",
    "    # Alle Daten welche sich im Fenster befinden ausw√§hlen und f√ºr die EOF-Analyse time attributes = 'T' setzen\n",
    "    data = anomalies.sel(time = anomalies.time.dt.dayofyear.isin(doy_window))\n",
    "    data.coords['time'].attrs['axis'] = 'T'\n",
    "    \n",
    "    # Daten des aktuellen Tag ausw√§hlen -> f√ºr die PPC-Berechnung\n",
    "    data_doy = anomalies.sel(time = anomalies.time.dt.dayofyear.isin(day.dayofyear))\n",
    "\n",
    "    # F√ºr die Analyse ist es notwendig die Daten in das Iris Format zu wandeln:\n",
    "    data_iris = iris.cube.CubeList([data.r.to_iris(), data.q.to_iris(), data.msl.to_iris()]).merge() \n",
    "    \n",
    "    # EOF-Analyse starten:\n",
    "    solver = MultivariateEof(data_iris, weights=None)\n",
    "    \n",
    "    # PC berechnen, bis 90% der Varianz erkl√§rt sind\n",
    "    j = 0\n",
    "    while(True):\n",
    "        variance = solver.varianceFraction(neigs=j)\n",
    "        if(np.sum(variance.data) > 0.9):\n",
    "            break\n",
    "        j += 1\n",
    "    \n",
    "    # Gefunden PCs und EOFs ausw√§hlen:\n",
    "    pc = solver.pcs(npcs=j)\n",
    "    eof = solver.eofs(neofs=j)\n",
    "    \n",
    "    # R√ºcktransformation nach fertiger EOF-Analyse und Fertige Berechnung zur Liste hinzuf√ºgen -> PC f√ºr die Normberechnung sp√§ter noch notwenig! \n",
    "    calculated_pc = xr.DataArray.from_iris(pc).rename('PC')\n",
    "    out_pc.append(calculated_pc)\n",
    "    out_r.append(xr.DataArray.from_iris(eof[0]).rename('EOF_r'))\n",
    "    out_q.append(xr.DataArray.from_iris(eof[1]).rename('EOF_q'))\n",
    "    out_msl.append(xr.DataArray.from_iris(eof[2]).rename('EOF_msl'))\n",
    "   \n",
    "    # F√ºr die Analyse ist es auch hier wieder notwendig die Daten des aktuellen DOY in das Iris Format zu wandeln:\n",
    "    data_doy_iris = iris.cube.CubeList([data_doy.r.to_iris(), data_doy.q.to_iris(), data_doy.msl.to_iris()]).merge()    \n",
    "\n",
    "    # Berechnen der PPCs:\n",
    "    ppc = solver.projectField(data_doy_iris, neofs=j)\n",
    "\n",
    "    # R√ºcktransformation:\n",
    "    data_ppc = xr.DataArray.from_iris(ppc).rename('PPC')\n",
    "\n",
    "    # √úber alle Jahre iterieren:\n",
    "    for k in range(0, years):\n",
    "        \n",
    "        # Berechnen der Norm\n",
    "        norm = (( calculated_pc - data_ppc[k] )**2).sum(dim='pc')\n",
    "        \n",
    "        # Das akutelle Jahr aus dem Pool herausnehmen\n",
    "        norm = norm.sel(time = ~norm.time.dt.year.isin(data_ppc.time.to_series()[k].year))\n",
    "        kday = data_ppc.time.to_series().dt.strftime('%Y-%m-%d')[k]\n",
    "        \n",
    "        # Reihe im Ausgabe Array festlegen\n",
    "        row = np.where(out_analoga[:,0] == datetime.datetime.strptime(kday, '%Y-%m-%d'))[0][0]\n",
    "\n",
    "        #Suche nach dem Analogon f√ºr den aktuellen Tag (Anzahl oben festgelegt -> nr)\n",
    "        for l in range(1,nr+1):\n",
    "            \n",
    "            # Norm minimieren -> Analogen finden: kleinste Norm = beste √úbereinstimmung\n",
    "            mini = np.argmin(norm).values\n",
    "            \n",
    "            # Das k.te Jahr auslblenden und Analogon bestimmen\n",
    "            analogon = data.sel(time = ~data.time.dt.year.isin(data_ppc.time.to_series()[k].year)).time.to_series()[mini]\n",
    "            \n",
    "            # Analogon im Array ablegen\n",
    "            out_analoga[row][l] = analogon\n",
    "            \n",
    "            # Das bereits gefunden Minimum herausnehmen, um die weiteren finden zu k√∂nnen:\n",
    "            norm = norm.where(norm.values != norm[mini].values)\n",
    "\n",
    "    print(day)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Speichern der Ausgangsdaten:\n",
    "cloudpickle.dump( out_r, open( path + \"/EOFr_\" + analogyear + \".p\", \"wb\" ) )\n",
    "cloudpickle.dump( out_q, open( path + \"/EOFq_\" + analogyear + \".p\", \"wb\" ) )\n",
    "cloudpickle.dump( out_msl, open( path + \"/EOFmsl_\" + analogyear + \".p\", \"wb\" ) )\n",
    "cloudpickle.dump( out_analoga, open( path + \"/Analoga_\" + analogyear + \".p\", \"wb\" ) )\n",
    "cloudpickle.dump( out_pc, open( path + \"/PC_\" + analogyear + \".p\", \"wb\" ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die meiste Arbeit war das Verstehen der Aufgabenstellung und das Implementieren dieser. Auch die Konvertierung in Iris hat einige Fragen aufgeworfen, war aber durch Cf - Konformit√§t der ERA5-Daten nich so problematisch wie bei anderen Grupppen."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
